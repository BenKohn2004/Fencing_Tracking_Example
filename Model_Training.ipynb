{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMODAQlqFUmMDgE0+FvovIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenKohn2004/Fencing_Tracking_Example/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the \"Acceleration_Files\" directory\n",
        "acceleration_files_dir = '/content/drive/MyDrive/Acceleration_Files/'\n",
        "\n",
        "# Path to the \"clips.csv\" file\n",
        "clips_csv_path = '/content/drive/MyDrive/clips.csv'\n",
        "\n",
        "# Open the \"clips.csv\" file\n",
        "with open(clips_csv_path, 'r') as clips_file:\n",
        "    clips_reader = csv.DictReader(clips_file)\n",
        "    clips_data = list(clips_reader)  # Store the rows in a list\n",
        "\n",
        "    # Iterate over each CSV file in the \"Acceleration_Files\" directory\n",
        "    for filename in os.listdir(acceleration_files_dir):\n",
        "        if filename.endswith('.csv'):\n",
        "            # Path to the current acceleration CSV file\n",
        "            acceleration_file_path = os.path.join(acceleration_files_dir, filename)\n",
        "\n",
        "            # Extract the Middle_Frame value from the filename\n",
        "            middle_frame = filename.split('_')[2].split('.')[0]\n",
        "\n",
        "            # Find the corresponding row in the \"clips.csv\" file\n",
        "            touch = None\n",
        "            for row in clips_data:\n",
        "                if row['Middle_Frame'] == middle_frame:\n",
        "                    touch = row['Touch']\n",
        "                    break\n",
        "\n",
        "            # Skip the file if no matching touch value found\n",
        "            if touch is None:\n",
        "                continue\n",
        "\n",
        "            print(f\"Updating file: {filename} with Touch value: {touch}\")\n",
        "\n",
        "            # Create a temporary file to write the updated CSV data\n",
        "            temp_file_path = os.path.join(acceleration_files_dir, 'temp.csv')\n",
        "            with open(acceleration_file_path, 'r') as input_file, open(temp_file_path, 'w', newline='') as output_file:\n",
        "                reader = csv.reader(input_file)\n",
        "                writer = csv.writer(output_file)\n",
        "\n",
        "                # Read the header row\n",
        "                header = next(reader)\n",
        "                header.append('Touch')\n",
        "                writer.writerow(header)\n",
        "\n",
        "                # Write the updated data to the temporary file\n",
        "                for row in reader:\n",
        "                    row.append(touch)\n",
        "                    writer.writerow(row)\n",
        "\n",
        "            # Replace the original file with the updated file\n",
        "            os.remove(acceleration_file_path)\n",
        "            os.rename(temp_file_path, acceleration_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPeNElqM12X6",
        "outputId": "b541bfdc-b3dd-4546-f694-4113394c59e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Updating file: acceleration_output_771270.csv with Touch value: Left\n",
            "Updating file: acceleration_output_773070.csv with Touch value: Left\n",
            "Updating file: acceleration_output_774240.csv with Touch value: Right\n",
            "Updating file: acceleration_output_771720.csv with Touch value: Right\n",
            "Updating file: acceleration_output_763380.csv with Touch value: Right\n",
            "Updating file: acceleration_output_691530.csv with Touch value: Left\n",
            "Updating file: acceleration_output_773550.csv with Touch value: Right\n",
            "Updating file: acceleration_output_770340.csv with Touch value: Left\n",
            "Updating file: acceleration_output_770580.csv with Touch value: Right\n",
            "Updating file: acceleration_output_775560.csv with Touch value: Right\n",
            "Updating file: acceleration_output_765060.csv with Touch value: Left\n",
            "Updating file: acceleration_output_690390.csv with Touch value: Right\n",
            "Updating file: acceleration_output_691920.csv with Touch value: Left\n",
            "Updating file: acceleration_output_772200.csv with Touch value: Left\n",
            "Updating file: acceleration_output_686820.csv with Touch value: Left\n",
            "Updating file: acceleration_output_691140.csv with Touch value: Left\n",
            "Updating file: acceleration_output_767100.csv with Touch value: Left\n",
            "Updating file: acceleration_output_761490.csv with Touch value: Right\n",
            "Updating file: acceleration_output_631380.csv with Touch value: Right\n",
            "Updating file: acceleration_output_679080.csv with Touch value: Left\n",
            "Updating file: acceleration_output_690660.csv with Touch value: Right\n",
            "Updating file: acceleration_output_633420.csv with Touch value: Right\n",
            "Updating file: acceleration_output_632610.csv with Touch value: Right\n",
            "Updating file: acceleration_output_679830.csv with Touch value: Right\n",
            "Updating file: acceleration_output_681480.csv with Touch value: Left\n",
            "Updating file: acceleration_output_767550.csv with Touch value: Right\n",
            "Updating file: acceleration_output_762030.csv with Touch value: Left\n",
            "Updating file: acceleration_output_686490.csv with Touch value: Right\n",
            "Updating file: acceleration_output_677370.csv with Touch value: Right\n",
            "Updating file: acceleration_output_760500.csv with Touch value: Right\n",
            "Updating file: acceleration_output_768270.csv with Touch value: Right\n",
            "Updating file: acceleration_output_774750.csv with Touch value: Right\n",
            "Updating file: acceleration_output_679500.csv with Touch value: Left\n",
            "Updating file: acceleration_output_687240.csv with Touch value: Right\n",
            "Updating file: acceleration_output_692490.csv with Touch value: Left\n",
            "Updating file: acceleration_output_685410.csv with Touch value: Left\n",
            "Updating file: acceleration_output_762960.csv with Touch value: Right\n",
            "Updating file: acceleration_output_772650.csv with Touch value: Right\n",
            "Updating file: acceleration_output_630960.csv with Touch value: Left\n",
            "Updating file: acceleration_output_677850.csv with Touch value: Left\n",
            "Updating file: acceleration_output_680250.csv with Touch value: Left\n",
            "Updating file: acceleration_output_678180.csv with Touch value: Left\n",
            "Updating file: acceleration_output_620280.csv with Touch value: Left\n",
            "Updating file: acceleration_output_677100.csv with Touch value: Left\n",
            "Updating file: acceleration_output_677640.csv with Touch value: Right\n",
            "Updating file: acceleration_output_623940.csv with Touch value: Right\n",
            "Updating file: acceleration_output_619680.csv with Touch value: Right\n",
            "Updating file: acceleration_output_629280.csv with Touch value: Right\n",
            "Updating file: acceleration_output_627630.csv with Touch value: Left\n",
            "Updating file: acceleration_output_683820.csv with Touch value: Right\n",
            "Updating file: acceleration_output_685920.csv with Touch value: Left\n",
            "Updating file: acceleration_output_621360.csv with Touch value: Right\n",
            "Updating file: acceleration_output_634560.csv with Touch value: Left\n",
            "Updating file: acceleration_output_621840.csv with Touch value: Left\n",
            "Updating file: acceleration_output_678510.csv with Touch value: Left\n",
            "Updating file: acceleration_output_628380.csv with Touch value: Left\n",
            "Updating file: acceleration_output_627210.csv with Touch value: Left\n",
            "Updating file: acceleration_output_619080.csv with Touch value: Left\n",
            "Updating file: acceleration_output_632100.csv with Touch value: Left\n",
            "Updating file: acceleration_output_684330.csv with Touch value: Right\n",
            "Updating file: acceleration_output_633870.csv with Touch value: Left\n",
            "Updating file: acceleration_output_630600.csv with Touch value: Right\n",
            "Updating file: acceleration_output_590250.csv with Touch value: Right\n",
            "Updating file: acceleration_output_592140.csv with Touch value: Left\n",
            "Updating file: acceleration_output_597720.csv with Touch value: Right\n",
            "Updating file: acceleration_output_599610.csv with Touch value: Left\n",
            "Updating file: acceleration_output_596430.csv with Touch value: Right\n",
            "Updating file: acceleration_output_588090.csv with Touch value: Left\n",
            "Updating file: acceleration_output_600090.csv with Touch value: Left\n",
            "Updating file: acceleration_output_619320.csv with Touch value: Right\n",
            "Updating file: acceleration_output_591570.csv with Touch value: Left\n",
            "Updating file: acceleration_output_592980.csv with Touch value: Right\n",
            "Updating file: acceleration_output_597300.csv with Touch value: Left\n",
            "Updating file: acceleration_output_600660.csv with Touch value: Left\n",
            "Updating file: acceleration_output_589530.csv with Touch value: Right\n",
            "Updating file: acceleration_output_598110.csv with Touch value: Right\n",
            "Updating file: acceleration_output_600990.csv with Touch value: Right\n",
            "Updating file: acceleration_output_623520.csv with Touch value: Right\n",
            "Updating file: acceleration_output_592710.csv with Touch value: Left\n",
            "Updating file: acceleration_output_626850.csv with Touch value: Right\n",
            "Updating file: acceleration_output_598620.csv with Touch value: Left\n",
            "Updating file: acceleration_output_622950.csv with Touch value: Right\n",
            "Updating file: acceleration_output_593700.csv with Touch value: Right\n",
            "Updating file: acceleration_output_620850.csv with Touch value: Right\n",
            "Updating file: acceleration_output_602160.csv with Touch value: Left\n",
            "Updating file: acceleration_output_622380.csv with Touch value: Right\n",
            "Updating file: acceleration_output_601500.csv with Touch value: Left\n",
            "Updating file: acceleration_output_527040.csv with Touch value: Right\n",
            "Updating file: acceleration_output_596730.csv with Touch value: Right\n",
            "Updating file: acceleration_output_591150.csv with Touch value: Right\n",
            "Updating file: acceleration_output_599160.csv with Touch value: Right\n",
            "Updating file: acceleration_output_515370.csv with Touch value: Right\n",
            "Updating file: acceleration_output_521340.csv with Touch value: Right\n",
            "Updating file: acceleration_output_520590.csv with Touch value: Left\n",
            "Updating file: acceleration_output_524460.csv with Touch value: Left\n",
            "Updating file: acceleration_output_519300.csv with Touch value: Left\n",
            "Updating file: acceleration_output_519630.csv with Touch value: Left\n",
            "Updating file: acceleration_output_593310.csv with Touch value: Right\n",
            "Updating file: acceleration_output_587850.csv with Touch value: Right\n",
            "Updating file: acceleration_output_525450.csv with Touch value: Left\n",
            "Updating file: acceleration_output_587640.csv with Touch value: Right\n",
            "Updating file: acceleration_output_520140.csv with Touch value: Left\n",
            "Updating file: acceleration_output_527610.csv with Touch value: Left\n",
            "Updating file: acceleration_output_589890.csv with Touch value: Left\n",
            "Updating file: acceleration_output_523890.csv with Touch value: Left\n",
            "Updating file: acceleration_output_519000.csv with Touch value: Right\n",
            "Updating file: acceleration_output_493470.csv with Touch value: Left\n",
            "Updating file: acceleration_output_482310.csv with Touch value: Left\n",
            "Updating file: acceleration_output_509790.csv with Touch value: Left\n",
            "Updating file: acceleration_output_523380.csv with Touch value: Left\n",
            "Updating file: acceleration_output_511380.csv with Touch value: Left\n",
            "Updating file: acceleration_output_494610.csv with Touch value: Right\n",
            "Updating file: acceleration_output_506580.csv with Touch value: Right\n",
            "Updating file: acceleration_output_495900.csv with Touch value: Left\n",
            "Updating file: acceleration_output_490860.csv with Touch value: Left\n",
            "Updating file: acceleration_output_511050.csv with Touch value: Right\n",
            "Updating file: acceleration_output_526020.csv with Touch value: Right\n",
            "Updating file: acceleration_output_514890.csv with Touch value: Right\n",
            "Updating file: acceleration_output_498000.csv with Touch value: Right\n",
            "Updating file: acceleration_output_507300.csv with Touch value: Left\n",
            "Updating file: acceleration_output_492240.csv with Touch value: Right\n",
            "Updating file: acceleration_output_510300.csv with Touch value: Left\n",
            "Updating file: acceleration_output_483510.csv with Touch value: Left\n",
            "Updating file: acceleration_output_512850.csv with Touch value: Left\n",
            "Updating file: acceleration_output_482700.csv with Touch value: Right\n",
            "Updating file: acceleration_output_496650.csv with Touch value: Right\n",
            "Updating file: acceleration_output_491310.csv with Touch value: Right\n",
            "Updating file: acceleration_output_491790.csv with Touch value: Left\n",
            "Updating file: acceleration_output_492870.csv with Touch value: Right\n",
            "Updating file: acceleration_output_479940.csv with Touch value: Left\n",
            "Updating file: acceleration_output_480660.csv with Touch value: Left\n",
            "Updating file: acceleration_output_480270.csv with Touch value: Right\n",
            "Updating file: acceleration_output_500100.csv with Touch value: Left\n",
            "Updating file: acceleration_output_473160.csv with Touch value: Left\n",
            "Updating file: acceleration_output_471570.csv with Touch value: Left\n",
            "Updating file: acceleration_output_587430.csv with Touch value: Left\n",
            "Updating file: acceleration_output_512400.csv with Touch value: Right\n",
            "Updating file: acceleration_output_472680.csv with Touch value: Right\n",
            "Updating file: acceleration_output_490470.csv with Touch value: Right\n",
            "Updating file: acceleration_output_500520.csv with Touch value: Left\n",
            "Updating file: acceleration_output_508860.csv with Touch value: Left\n",
            "Updating file: acceleration_output_478740.csv with Touch value: Left\n",
            "Updating file: acceleration_output_471810.csv with Touch value: Right\n",
            "Updating file: acceleration_output_468030.csv with Touch value: Left\n",
            "Updating file: acceleration_output_467370.csv with Touch value: Left\n",
            "Updating file: acceleration_output_472470.csv with Touch value: Left\n",
            "Updating file: acceleration_output_472920.csv with Touch value: Left\n",
            "Updating file: acceleration_output_472080.csv with Touch value: Left\n",
            "Updating file: acceleration_output_479520.csv with Touch value: Left\n",
            "Updating file: acceleration_output_443490.csv with Touch value: Left\n",
            "Updating file: acceleration_output_466800.csv with Touch value: Left\n",
            "Updating file: acceleration_output_447810.csv with Touch value: Left\n",
            "Updating file: acceleration_output_478950.csv with Touch value: Left\n",
            "Updating file: acceleration_output_481590.csv with Touch value: Left\n",
            "Updating file: acceleration_output_450480.csv with Touch value: Right\n",
            "Updating file: acceleration_output_490110.csv with Touch value: Right\n",
            "Updating file: acceleration_output_450180.csv with Touch value: Right\n",
            "Updating file: acceleration_output_444720.csv with Touch value: Right\n",
            "Updating file: acceleration_output_450810.csv with Touch value: Right\n",
            "Updating file: acceleration_output_471360.csv with Touch value: Left\n",
            "Updating file: acceleration_output_468870.csv with Touch value: Left\n",
            "Updating file: acceleration_output_449310.csv with Touch value: Left\n",
            "Updating file: acceleration_output_513330.csv with Touch value: Right\n",
            "Updating file: acceleration_output_466050.csv with Touch value: Left\n",
            "Updating file: acceleration_output_467010.csv with Touch value: Right\n",
            "Updating file: acceleration_output_449910.csv with Touch value: Right\n",
            "Updating file: acceleration_output_448710.csv with Touch value: Right\n",
            "Updating file: acceleration_output_465510.csv with Touch value: Left\n",
            "Updating file: acceleration_output_479250.csv with Touch value: Right\n",
            "Updating file: acceleration_output_466350.csv with Touch value: Left\n",
            "Updating file: acceleration_output_442200.csv with Touch value: Left\n",
            "Updating file: acceleration_output_451110.csv with Touch value: Left\n",
            "Updating file: acceleration_output_448980.csv with Touch value: Right\n",
            "Updating file: acceleration_output_451350.csv with Touch value: Right\n",
            "Updating file: acceleration_output_344400.csv with Touch value: Right\n",
            "Updating file: acceleration_output_445440.csv with Touch value: Right\n",
            "Updating file: acceleration_output_442440.csv with Touch value: Left\n",
            "Updating file: acceleration_output_343590.csv with Touch value: Right\n",
            "Updating file: acceleration_output_442680.csv with Touch value: Right\n",
            "Updating file: acceleration_output_443760.csv with Touch value: Left\n",
            "Updating file: acceleration_output_347730.csv with Touch value: Left\n",
            "Updating file: acceleration_output_443220.csv with Touch value: Left\n",
            "Updating file: acceleration_output_345210.csv with Touch value: Left\n",
            "Updating file: acceleration_output_441810.csv with Touch value: Right\n",
            "Updating file: acceleration_output_760740.csv with Touch value: Right\n",
            "Updating file: acceleration_output_444510.csv with Touch value: Right\n",
            "Updating file: acceleration_output_522840.csv with Touch value: Right\n",
            "Updating file: acceleration_output_343920.csv with Touch value: Left\n",
            "Updating file: acceleration_output_442920.csv with Touch value: Left\n",
            "Updating file: acceleration_output_344820.csv with Touch value: Right\n",
            "Updating file: acceleration_output_441600.csv with Touch value: Right\n",
            "Updating file: acceleration_output_449610.csv with Touch value: Left\n",
            "Updating file: acceleration_output_338190.csv with Touch value: Right\n",
            "Updating file: acceleration_output_346860.csv with Touch value: Right\n",
            "Updating file: acceleration_output_337500.csv with Touch value: Left\n",
            "Updating file: acceleration_output_337140.csv with Touch value: Right\n",
            "Updating file: acceleration_output_336060.csv with Touch value: Left\n",
            "Updating file: acceleration_output_338520.csv with Touch value: Right\n",
            "Updating file: acceleration_output_334980.csv with Touch value: Left\n",
            "Updating file: acceleration_output_342240.csv with Touch value: Left\n",
            "Updating file: acceleration_output_337890.csv with Touch value: Right\n",
            "Updating file: acceleration_output_334500.csv with Touch value: Right\n",
            "Updating file: acceleration_output_339900.csv with Touch value: Left\n",
            "Updating file: acceleration_output_444150.csv with Touch value: Right\n",
            "Updating file: acceleration_output_465840.csv with Touch value: Left\n",
            "Updating file: acceleration_output_347250.csv with Touch value: Right\n",
            "Updating file: acceleration_output_335280.csv with Touch value: Right\n",
            "Updating file: acceleration_output_343170.csv with Touch value: Right\n",
            "Updating file: acceleration_output_338820.csv with Touch value: Left\n",
            "Updating file: acceleration_output_336480.csv with Touch value: Left\n",
            "Updating file: acceleration_output_348240.csv with Touch value: Right\n",
            "Updating file: acceleration_output_342840.csv with Touch value: Right\n",
            "Updating file: acceleration_output_334710.csv with Touch value: Left\n",
            "Updating file: acceleration_output_507780.csv with Touch value: Right\n",
            "Updating file: acceleration_output_441390.csv with Touch value: Right\n",
            "Updating file: acceleration_output_335700.csv with Touch value: Left\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the \"Acceleration_Files\" directory\n",
        "acceleration_files_dir = '/content/drive/MyDrive/Acceleration_Files/'\n",
        "\n",
        "# Iterate over each CSV file in the \"Acceleration_Files\" directory\n",
        "for filename in os.listdir(acceleration_files_dir):\n",
        "    if filename.endswith('.csv'):\n",
        "        # Path to the current acceleration CSV file\n",
        "        acceleration_file_path = os.path.join(acceleration_files_dir, filename)\n",
        "\n",
        "        # Read the contents of the current CSV file\n",
        "        with open(acceleration_file_path, 'r') as file:\n",
        "            reader = csv.reader(file)\n",
        "            rows = list(reader)\n",
        "\n",
        "        # Remove the 5th and 6th columns from each row\n",
        "        updated_rows = [row[:4] + row[6:] for row in rows]\n",
        "\n",
        "        # Write the updated data back to the CSV file\n",
        "        with open(acceleration_file_path, 'w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerows(updated_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqIOQijC4QRD",
        "outputId": "8b963bb8-d14f-436f-bc58-da6d8e8f8198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/MyDrive/Acceleration_Files/acceleration_output_334500.csv'\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Print the loaded data\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "G74hOxKjUh8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing the CSV files\n",
        "directory = '/content/drive/MyDrive/Acceleration_Files'\n",
        "\n",
        "# Iterate over CSV files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Read CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check the count of \"1\" in the \"Green_Light\" column\n",
        "        green_light_count = df['Green_Light'].sum()\n",
        "        if green_light_count > 15:\n",
        "            # Set all values in the \"Green_Light\" column to 0\n",
        "            df['Green_Light'] = 0\n",
        "\n",
        "        # Check the count of \"1\" in the \"Red_Light\" column\n",
        "        red_light_count = df['Red_Light'].sum()\n",
        "        if red_light_count > 15:\n",
        "            # Set all values in the \"Red_Light\" column to 0\n",
        "            df['Red_Light'] = 0\n",
        "\n",
        "        # Save the modified DataFrame back to the CSV file\n",
        "        df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uP5Av4qq7ad",
        "outputId": "6d5736ea-f9f7-49fd-f805-69cc4f633d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory paths\n",
        "acceleration_dir = '/content/drive/MyDrive/Acceleration_Files'\n",
        "output_dir = '/content/drive/MyDrive/Acceleration_Files'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Iterate over the CSV files in the acceleration directory\n",
        "for file in os.listdir(acceleration_dir):\n",
        "    if file.endswith('.csv'):\n",
        "        # Read the original CSV file\n",
        "        file_path = os.path.join(acceleration_dir, file)\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Perform the column swaps\n",
        "        df['Left'], df['Right'] = df['Right'], df['Left']\n",
        "        df['Red_Light'], df['Green_Light'] = df['Green_Light'], df['Red_Light']\n",
        "        df['Touch'] = df['Touch'].map({'Left': 'Right', 'Right': 'Left'})\n",
        "\n",
        "        # Save the modified CSV file with the \"_mirror\" suffix\n",
        "        output_file = os.path.join(output_dir, file.replace('.csv', '_mirror.csv'))\n",
        "        df.to_csv(output_file, index=False)\n",
        "\n",
        "        print(f\"Processed file: {file} -> Saved as: {output_file}\")\n"
      ],
      "metadata": {
        "id": "YaKt6ZfQkkqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Model Training"
      ],
      "metadata": {
        "id": "rdK_xbe4K3KN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vdvWxeuUN_k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing the CSV files\n",
        "directory = '/content/drive/MyDrive/Acceleration_Files'\n",
        "\n",
        "# Load CSV files and preprocess the data\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "# Iterate over CSV files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Read CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Extract input sequences\n",
        "        sequence = df[['Left', 'Right', 'Red_Light', 'Green_Light']].values\n",
        "\n",
        "        # Extract target variable\n",
        "        target_data = df['Touch'].values[0]  # Assuming the target is the same for all rows\n",
        "        target = 1 if target_data == 'Right' else 0\n",
        "\n",
        "        sequences.append(sequence)\n",
        "        targets.append(target)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "sequences = pad_sequences(sequences, dtype='float32', padding='post')\n",
        "\n",
        "# Convert target list to NumPy array\n",
        "targets = np.array(targets)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the GRU model\n",
        "model = Sequential()\n",
        "model.add(GRU(64, input_shape=(None, 4)))  # Assuming variable-length input with 4 features\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=600, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Model"
      ],
      "metadata": {
        "id": "eXnYQAeO00Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.transformers import Transformer\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing the CSV files\n",
        "directory = '/content/drive/MyDrive/Acceleration_Files'\n",
        "\n",
        "# Load CSV files and preprocess the data\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# Iterate over CSV files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Read CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Extract input features\n",
        "        features.extend(df[['Left', 'Right', 'Red_Light', 'Green_Light']].values)\n",
        "\n",
        "        # Extract target variable (assuming 'Touch' column contains the target labels)\n",
        "        target_data = df['Touch'].values[0]  # Assuming the target is the same for all rows\n",
        "        target.extend([target_data] * len(df))  # Repeat target for each row in the CSV file\n",
        "\n",
        "# Convert feature and target lists to NumPy arrays\n",
        "X = np.array(features)\n",
        "y = np.where(np.array(target) == 'Left', 0, 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Transformer model\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = 2\n",
        "\n",
        "inputs = Input(shape=(input_dim,))\n",
        "transformer_layer = Transformer(num_heads=2, feed_forward_dim=32, num_transformer_blocks=2)(inputs)\n",
        "x = Dropout(0.2)(transformer_layer)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss=BinaryCrossentropy(),\n",
        "              metrics=[Accuracy()])\n",
        "\n",
        "# Train the model\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "model.fit(X_train, to_categorical(y_train), validation_data=(X_test, to_categorical(y_test)),\n",
        "          batch_size=32, epochs=20, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, to_categorical(y_test))\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "peKgMQVX0fyY",
        "outputId": "9b51ae3c-9447-48e9-dabb-de5f02225531"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3865717086e8>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing the CSV files\n",
        "directory = '/content/drive/MyDrive/Acceleration_Files'\n",
        "\n",
        "# Load CSV files and preprocess the data\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# Iterate over CSV files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Read CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Extract input features\n",
        "        features.extend(df[['Left', 'Right', 'Red_Light', 'Green_Light']].values)\n",
        "\n",
        "        # Extract target variable (assuming 'Touch' column contains the target labels)\n",
        "        target_data = df['Touch'].values[0]  # Assuming the target is the same for all rows\n",
        "        target.extend([target_data] * len(df))  # Repeat target for each row in the CSV file\n",
        "\n",
        "# Convert feature and target lists to NumPy arrays\n",
        "X = np.array(features)\n",
        "y = np.where(np.array(target) == 'Left', 0, 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Transformer model\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = 2\n",
        "dropout_rate = 0.2\n",
        "num_heads = 2\n",
        "feed_forward_dim = 32\n",
        "num_transformer_blocks = 2\n",
        "\n",
        "inputs = Input(shape=(input_dim,))\n",
        "x = inputs\n",
        "\n",
        "for _ in range(num_transformer_blocks):\n",
        "    # Multi-Head Attention\n",
        "    x = MultiHeadAttention(num_heads=num_heads, key_dim=feed_forward_dim)(x, x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # Feed-Forward Neural Network\n",
        "    x = Dense(feed_forward_dim, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss=BinaryCrossentropy(),\n",
        "              metrics=[Accuracy()])\n",
        "\n",
        "# Train the model\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "model.fit(X_train, to_categorical(y_train), validation_data=(X_test, to_categorical(y_test)),\n",
        "          batch_size=32, epochs=20, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, to_categorical(y_test))\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "_3QpKuR-1MjW",
        "outputId": "8ca51396-a4fe-4452-aa7e-a005e998d493"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c4d0f515ad78>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_transformer_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Multi-Head Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_forward_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/layers/activation/softmax.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 )\n\u001b[1;32m    102\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Exception encountered when calling layer 'softmax' (type Softmax).\n\ntuple index out of range\n\nCall arguments received by layer 'softmax' (type Softmax):\n  • inputs=tf.Tensor(shape=(None, 2), dtype=float32)\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU Model"
      ],
      "metadata": {
        "id": "4wnYX65e1Wj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing the CSV files\n",
        "directory = '/content/drive/MyDrive/Acceleration_Files'\n",
        "\n",
        "# Load CSV files and preprocess the data\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "# Iterate over CSV files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Read CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Extract input sequences\n",
        "        sequence = df[['Left', 'Right', 'Red_Light', 'Green_Light']].values\n",
        "\n",
        "        # Extract target variable\n",
        "        target_data = df['Touch'].values[0]  # Assuming the target is the same for all rows\n",
        "        target = 1 if target_data == 'Right' else 0\n",
        "\n",
        "        sequences.append(sequence)\n",
        "        targets.append(target)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "sequences = pad_sequences(sequences, dtype='float32', padding='post')\n",
        "\n",
        "# Convert target list to NumPy array\n",
        "targets = np.array(targets)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the GRU model\n",
        "model = Sequential()\n",
        "model.add(GRU(64, input_shape=(None, 4)))  # Assuming variable-length input with 4 features\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=250, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save the model to Google Drive\n",
        "model.save('/content/drive/MyDrive/model.h5')\n"
      ],
      "metadata": {
        "id": "FhQStAx81Y7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory containing the CSV files\n",
        "directory = '/content/drive/MyDrive/Acceleration_Files'\n",
        "\n",
        "# List of CSV files to analyze\n",
        "csv_files = [\"acceleration_Clip 1.csv\", \"acceleration_Clip 2.csv\", \"acceleration_Clip 3.csv\"]\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/drive/MyDrive/model.h5')\n",
        "\n",
        "# Iterate over CSV files\n",
        "for csv_file in csv_files:\n",
        "    csv_path = os.path.join(directory, csv_file)\n",
        "\n",
        "    # Read CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Extract input sequences\n",
        "    sequences = df[['Left', 'Right', 'Red_Light', 'Green_Light']].values\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    sequences = pad_sequences([sequences], dtype='float32', padding='post')\n",
        "\n",
        "    # Perform inference using the loaded model\n",
        "    predictions = model.predict(sequences)\n",
        "\n",
        "    # Print the predictions\n",
        "    print(f\"Predictions for {csv_file}:\")\n",
        "    for i, pred in enumerate(predictions):\n",
        "        touch = \"Right Touch\" if pred[0] > 0.5 else \"Left Touch\"\n",
        "        percentage = pred[0] if touch == \"Right Touch\" else 1 - pred[0]\n",
        "        print(f\"Sequence {i+1}: {touch}, Probability: {percentage}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJFNnAYC-z8z",
        "outputId": "53a59a0f-e125-46b3-ac1f-8446ad3ab021"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "1/1 [==============================] - 1s 786ms/step\n",
            "Predictions for acceleration_Clip 1.csv:\n",
            "Sequence 1: Right Touch, Probability: 0.9849115610122681\n",
            "\n",
            "1/1 [==============================] - 1s 773ms/step\n",
            "Predictions for acceleration_Clip 2.csv:\n",
            "Sequence 1: Left Touch, Probability: 0.9853239255025983\n",
            "\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "Predictions for acceleration_Clip 3.csv:\n",
            "Sequence 1: Left Touch, Probability: 0.9996226709918119\n",
            "\n"
          ]
        }
      ]
    }
  ]
}